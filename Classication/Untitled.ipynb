{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6\n",
    "\n",
    "# Training and Cost Function\n",
    "\n",
    "Bien, ahora sabes cómo un modelo de regresión logística estima las probabilidades y hace predicciones.¿Pero cómo se entrena? El objetivo del entrenamiento es establecer el vector de parámetros $θ$ para que el modelo estime probabilidades altas para casos positivos $(y = 1)$ y bajas probabilidades para casos negativos $(y = 0)$. \n",
    "La función de costo en todo el conjunto de entrenamiento es simplemente el costo promedio de todo el entrenamiento. En instancias. Se puede escribir en una sola expresión (como se puede verificar fácilmente), llamada la pérdida de registro, que se muestra en la ecuación .\n",
    "\n",
    "Logistic Regression cost function (log loss)\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}log\\bigg(\\hat{p}^{(i)}\\bigg)+\\bigg(1−y^{(i)}\\bigg)log\\bigg(1−\\hat{p}^{(i)}\\bigg)$$\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}log\\bigg(h_{\\theta}(x^{(i)})\\bigg)+\\bigg(1−y^{(i)}\\bigg)log\\bigg(1−h_\\theta(x^{(i)})\\bigg)$$\n",
    "\n",
    "la mala noticia es que no se conoce una ecuación de forma cerrada para calcular el valor de $θ$ que minimiza esta función de costo (no hay equivalente de la ecuación normal). Pero la buena noticia es que esta función de costo es convexa, por lo que el Gradient Descent (o cualquier otro) otro algoritmo de optimización) está garantizado para encontrar el mínimo global (si el aprendizaje La tasa de entrada no es demasiado grande y se espera el tiempo suficiente).\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{\\partial }{\\partial \\theta_j} -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}log\\bigg(h_\\theta(x^{(i)})\\bigg)+\\bigg(1−y^{(i)}\\bigg)log\\bigg(1−h_\\theta(x^{(i)})\\bigg)$$\n",
    "\n",
    "por linealidad \n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\\frac{\\partial }{\\partial \\theta_j}log\\bigg(h_\\theta(x^{(i)})\\bigg)+\\bigg(1−y^{(i)}\\bigg)\\frac{\\partial }{\\partial \\theta_j}log\\bigg(1−h_\\theta(x^{(i)})\\bigg)\\bigg]$$\n",
    "\n",
    "al utiliza regla de la cadena\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\\frac{\\frac{\\partial }{\\partial \\theta_j}h_\\theta(x^{(i)})}{h_\\theta(x^{(i)})}+\\bigg(1−y^{(i)}\\bigg)\\frac{\\frac{\\partial }{\\partial \\theta_j}\\bigg(1−h_\\theta(x^{(i)})\\bigg)}{1−h_\\theta(x^{(i)})}\\bigg]$$\n",
    "\n",
    "$$h_\\theta(x)=\\sigma(\\theta^⊤ x^{(i)})$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "\\frac{\\frac{\\partial }{\\partial \\theta_j}\\sigma(\\theta^⊤ x^{(i)})}{\\sigma(\\theta^⊤x^{(i)})}+\\bigg(1−y^{(i)}\\bigg)\\frac{\\frac{\\partial }{\\partial \\theta_j}\\bigg(1−\\sigma(\\theta^⊤ x^{(i)})\\bigg)} {1−\\sigma(\\theta^⊤ x^{(i)})}\\bigg]$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\\frac{\\sigma(\\theta^⊤ x^{(i)})(1-\\sigma(\\theta^⊤ x^{(i)}))\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})}\n",
    "{\\sigma(\\theta^⊤x^{(i)})}-\\bigg(1−y^{(i)}\\bigg)\\frac{\\sigma(\\theta^⊤ x^{(i)})(1-\\sigma(\\theta^⊤ x^{(i)}))\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})}\n",
    "{1-\\sigma(\\theta^⊤x^{(i)})}\\bigg]$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\\frac{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})}\n",
    "{h_\\theta(x^{(i)})}-\\bigg(1−y^{(i)}\\bigg)\\frac{h_\\theta(x^{(i)})(1-h_\\theta(x^{(i)}))\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})}\n",
    "{1-h_\\theta(x^{(i)})}\\bigg]$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "(1-h_\\theta(x^{(i)}))\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})\n",
    "-\\bigg(1−y^{(i)}\\bigg)h_\\theta(x^{(i)})\\frac{\\partial }{\\partial \\theta_j}(\\theta^⊤ x^{(i)})\n",
    "\\bigg]$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "(1-h_\\theta(x^{(i)})) x^{(i)}_j\n",
    "-\\bigg(1−y^{(i)}\\bigg)h_\\theta(x^{(i)})) x^{(i)}_j\\bigg]$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "(1-h_\\theta(x^{(i)}))\n",
    "-\\bigg(1−y^{(i)}\\bigg)h_\\theta(x^{(i)})\\bigg]x^{(i)}_j$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "-y^{(i)}h_\\theta(x^{(i)})\n",
    "-h_\\theta(x^{(i)})+y^{(i)}h_\\theta(x^{(i)})\\bigg]x^{(i)}_j$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= -\\frac{1}{m}\\sum_{i=1}^{m} \\bigg[y^{(i)}\n",
    "-h_\\theta(x^{(i)})\\bigg]x^{(i)}_j$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j}= \\frac{1}{m}\\sum_{i=1}^{m} \\bigg[\n",
    "\\sigma(\\theta^⊤ x^{(i)})-y^{(i)}\\bigg]x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy \n",
    "La entropía cruzada se originó a partir de la teoría de la información. Supongamos que usted quiere eficientemente Transmite información sobre el clima todos los días. Si hay ocho opciones (soleado, lluvioso, etc.), podría codificar cada opción usando 3 bits desde 23 = 8. Sin embargo, si Creo que estará soleado casi todos los días, sería mucho más eficiente codificar \"Soleado\" en un solo bit (0) y las otras siete opciones en 4 bits (comenzando con un 1). La entropía cruzada mide el número promedio de bits que realmente envía por opción. Si su suposición sobre el clima es perfecta, la entropía cruzada será igual a la La entropía del clima en sí (es decir, su imprevisibilidad intrínseca). Pero si tu asume- Las condiciones son incorrectas (por ejemplo, si llueve con frecuencia), la entropía cruzada será mayor en una cantidad Llamado la divergencia Kullback-Leibler.\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k log(\\hat{p}^{(i)}_k) $$\n",
    "$$\\hat{p}^{(i)}_k=\\frac{exp({\\theta^⊤_k x^{(i)})}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}} $$\n",
    "\n",
    "$$log(\\hat{p}^{(i)}_k)=log\\bigg(\\frac{exp({\\theta^⊤_k x^{(i)})}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$log(\\hat{p}^{(i)}_k)=log (exp({\\theta^⊤_k x^{(i)})})-log{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "$$log(\\hat{p}^{(i)}_k)={\\theta^⊤_k x^{(i)}}-log{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({\\theta^⊤_k x^{(i)}}-log{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\frac{\\partial }{\\partial \\theta_{j\\ell}}\\bigg({\\theta^⊤_k x^{(i)}}-log{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{\\partial }{\\partial \\theta_{j\\ell}}log{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{\\frac{\\partial }{\\partial \\theta_{j\\ell}}\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{\\sum_{k=1}^{k}\\frac{\\partial }{\\partial \\theta_{j\\ell}}exp({\\theta^⊤_k x^{(i)})}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})\\frac{\\partial }{\\partial \\theta_{j\\ell}}(\\theta^⊤_k x^{(i)})}}{\\sum_{k=1}^{k}exp({\\theta^⊤_\\alpha x^{(i)})}}\\bigg)$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})(\\delta_{kj} x^{(i)}_\\ell)}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-\\frac{exp({\\theta^⊤_j x^{(i)})(x^{(i)}_\\ell)}}{\\sum_{k=1}^{k}exp({\\theta^⊤_k x^{(i)})}}\\bigg)$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} x^{(i)}_\\ell}-x^{(i)}_\\ell\\hat{p}^{(i)}_j\\bigg)$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{k}y^{(i)}_k \\bigg({ \\delta_{kj} }-\\hat{p}^{(i)}_j\\bigg)x^{(i)}_\\ell$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= -\\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}_\\ell\\sum_{k=1}^{k}y^{(i)}_k \\bigg[{ \\delta_{kj} }-\\hat{p}^{(i)}_j\\bigg]$$\n",
    "\n",
    "\n",
    "$$ \\sum_{k=1}^{k}y^{(i)}_k \\bigg[{ \\delta_{kj} }-\\hat{p}^{(i)}_j\\bigg]=y^{(i)}_j \\bigg[\\hat{p}^{(i)}_j-1\\bigg]+\\sum_{k\\neq j} y^{(i)}_k \\hat{p}^{(i)}_j=\\hat{p}^{(i)}_j\\bigg(y^{(i)}_j+\\sum_{k\\neq j} y^{(i)}_k\\bigg)-y^{(i)}_j$$\n",
    "\n",
    "$y$ Es un vector codificado  para las etiquetas, asi que $\\sum_{k} y^{(i)}_k=1 $and $y^{(i)}_j+\\sum_{k\\neq j} y^{(i)}_k=1$.Entonces tenemos\n",
    "$$ \\sum_{k=1}^{k}y^{(i)}_k \\bigg[{ \\delta_{kj} }-\\hat{p}^{(i)}_j\\bigg]=\\hat{p}^{(i)}_j-y^{(i)}_j$$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_{j\\ell}}= \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}\\bigg[\\hat{p}^{(i)}_j-y^{(i)}_j\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "El clasificador Naive Bayes agrega información usando la probabilidad condicional con un supuesto de independencia entre las características . Qué significa eso? Por ejemplo, significa que debemos asumir que los años de un jugador son independientes de su efectividad en el campo de fútbol. Esta suposición es absolutamente errónea y es por eso que se llama ingenua . Permite simplificar el cálculo, incluso en conjuntos de datos muy grandes. Veamos por qué.<a href=\"https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44\">[2]</a>\n",
    "\n",
    "El clasificador Naive Bayes se basa en la búsqueda de funciones que describen la probabilidad de pertenecer a una clase dada características . Lo escribimos $P( y | x_1,…, x_n)$. Aplicamos la ley de Bayes para simplificar el cálculo<a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes\">[2]</a>\n",
    "\n",
    "$$P( y | x_1,…, x_n)=\\frac{P(y)P(x_1,…, x_n|y)}{ P(x_1,…, x_n)}$$\n",
    "\n",
    "Usando la ingenua suposición de independencia condicional que\n",
    "\n",
    "$$P(x_i |y, x_1,…,x_{i-1},x_{i+1},…,x_n)=P(x_{i}| y)$$\n",
    "\n",
    "para todos i , esta relación se simplifica a\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 \n",
    "Ya que $P(x_1, \\dots, x_n)$ es constante dada la entrada, podemos usar la siguiente regla de clasificación:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "$$\\Downarrow$$\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),$$\n",
    "\n",
    "y podemos usar la estimación Máxima A Posteriori (MAP) para estimar$ P (y)$ y $P(x_i \\mid y)$; el primero es entonces la frecuencia relativa de la clase y en el conjunto de entrenamiento. \n",
    "\n",
    "Los diferentes clasificadores de Bayes ingenuos difieren principalmente por los supuestos que hacen con respecto a la distribución de $P(x_i \\mid y)$.\n",
    "\n",
    "NB : un error común es considerar las salidas de probabilidad del clasificador como verdaderas . De hecho, Naive Bayes es conocido como un mal estimador , por lo que no tome esos resultados probabilísticos demasiado en serio.<a href=\"https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44\">[2]</a>\n",
    "\n",
    "\n",
    "\n",
    "Queda un último paso para comenzar a implementar un clasificador. ¿Cómo modelar las funciones de probabilidad$ P (x_i  \\mid y)? Hay tres modelos disponibles en la biblioteca de Sklearn python:\n",
    "\n",
    "- Gauss: asume que las características continuas siguen una distribución normal .\n",
    "\n",
    "![alt-text-1](https://sites.google.com/site/nuclearremotelaboratory/_/rsrc/1395412565275/prepare-the-samples/lesson-1/exp-1-roadsite-plants/analyze-data/gaussian-distribution/bell%20curve.png)\n",
    "\n",
    "<h2 align=\"right\"></h2> \n",
    "\n",
    "- Multinomial : es útil si sus características son discretas.\n",
    "- Bernoulli : el modelo binomial es útil si sus características son binarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones básicas\n",
    "\n",
    "Preparar módulos de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"imagesAM\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST : \"Hello world\" de clasificación \n",
    "Se ordenan los datos para asegurar repetibilidad de los experimentos. Note que se ordenan por separado el conjunto de entrenamiento (primeras 60000 imágenes) y el conjunto de evaluación (las restantes 10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
    "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
    "    mnist.data[:60000] = mnist.data[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data[reorder_test + 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digit = X[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = mpl.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] #particionar train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(60000) #permutación datos\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo clasificador binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f7dfd3cc0c3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Importing dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Convert categorical variable to numeric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "# Importing dataset\n",
    "data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Convert categorical variable to numeric\n",
    "data[\"Sex_cleaned\"]=np.where(data[\"Sex\"]==\"male\",0,1)\n",
    "data[\"Embarked_cleaned\"]=np.where(data[\"Embarked\"]==\"S\",0,\n",
    "                                  np.where(data[\"Embarked\"]==\"C\",1,\n",
    "                                           np.where(data[\"Embarked\"]==\"Q\",2,3)\n",
    "                                          )\n",
    "                                 )\n",
    "# Cleaning dataset of NaN\n",
    "data=data[[\n",
    "    \"Survived\",\n",
    "    \"Pclass\",\n",
    "    \"Sex_cleaned\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked_cleaned\"\n",
    "]].dropna(axis=0, how='any')\n",
    "\n",
    "# Split dataset in training and test datasets\n",
    "X_train, X_test = train_test_split(data, test_size=0.5, random_state=int(time.time()))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
